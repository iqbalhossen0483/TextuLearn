# ğŸ“˜ **Product Requirements Document (PRD)**

## Project: AI Book Agent Platform (Flask + LangChain + Pinecone + Google ADK)

---

## ğŸ§­ Overview

The **AI Book Agent** is a Python-based backend system that provides intelligent interaction with the contents of a specific book. The system allows users to upload pre-chunked contextual data from a book and interact with it using multiple AI agents. It supports functionalities such as chatting about the book, generating short and long Q\&A, and selecting the best answerâ€”all powered by vector search (Pinecone), language models (Google Gemini via LangChain), and orchestrated via Googleâ€™s Agent Development Kit (ADK).

---

## ğŸ¯ Goals & Objectives

- Accept pre-split contextual chunks of a book and embed them.
- Store vector data in Pinecone for similarity-based retrieval.
- Provide a set of intelligent agents using Google ADK:

  - Chat agent
  - Short Q\&A generator
  - Long Q\&A generator
  - Best answer selector

- Expose functionality via a REST API built with **Flask**.
- Design a modular, scalable backend for future extension.

---

## ğŸ‘¥ Target Users

- Students, educators, and researchers
- AI-powered reading or learning apps
- Content summarization platforms
- Book analysis tools

---

## ğŸ— Architecture Overview

```
Frontend (splits book into contexts)
         |
         v
Backend (Flask API)
  â”œâ”€â”€ Receives context chunks
  â”œâ”€â”€ Embeds with LangChain
  â”œâ”€â”€ Stores in Pinecone
  â””â”€â”€ Routes to AI agents via Google ADK
```

---

## ğŸ”§ Tech Stack

| Purpose        | Tool/Library                  |
| -------------- | ----------------------------- |
| API Server     | Flask                         |
| Embedding      | LangChain (Gemini Embeddings) |
| Vector Store   | Pinecone                      |
| Agents         | Google Agent Development Kit  |
| LLM            | Google Gemini Pro             |
| Storage Format | JSON + Pinecone Metadata      |
| Deployment     | Gunicorn + Docker (optional)  |

---

## ğŸ›  Functional Requirements

### 1. **Upload Contextual Book Chunks**

- **Input**: Array of contextual text chunks from the frontend.
- **Backend Tasks**:

  - Embed each chunk using LangChain
  - Store each vector in Pinecone with `book_id`, chunk index, and metadata

#### `/upload-book`

- **Method**: `POST`
- **Payload**:

```json
{
  "book_id": "unique_book_id",
  "chunks": [
    "context chunk 1...",
    "context chunk 2...",
    ...
  ]
}
```

- **Response**: `{ "status": "success", "stored": <number_of_chunks> }`

---

### 2. **Chat with Book**

Use retriever to find top relevant chunks, then LLM to answer.

#### `/chat`

- **Method**: `POST`
- **Payload**:

```json
{
  "book_id": "unique_book_id",
  "question": "What happened to the main character?"
}
```

- **Response**: `{ "response": "..." }`

---

### 3. **Short Question-Answer Generator**

Generates short Q\&A pairs based on book context.

#### `/short-qa`

- **Method**: `POST`
- **Payload**:

```json
{
  "book_id": "unique_book_id",
  "topic": "Chapter 3 summary"
}
```

- **Response**:

```json
{
  "qa_pairs": [
    {
      "q": "What is the conflict in chapter 3?",
      "a": "The character discovers..."
    }
  ]
}
```

---

### 4. **Long Question-Answer Generator**

Provides a more detailed Q\&A response.

#### `/long-qa`

- **Method**: `POST`
- **Payload**:

```json
{
  "book_id": "unique_book_id",
  "topic": "Theme of betrayal in chapter 4"
}
```

- **Response**:

```json
{
  "question": "...",
  "answer": "In chapter 4, betrayal is shown when..."
}
```

---

### 5. **Best Answer Selector**

Given multiple answers, the agent selects the best one.

#### `/best-answer`

- **Method**: `POST`
- **Payload**:

```json
{
  "question": "Why did the hero leave the village?",
  "answers": [
    "To find the sword of truth",
    "To escape danger",
    "Because the elders told him to"
  ]
}
```

- **Response**:

```json
{
  "best_answer": "To find the sword of truth",
  "reason": "This directly relates to the main quest."
}
```

---

### 6. **Health Check**

#### `/status`

- **Method**: `GET`
- **Response**: `{ "status": "ok" }`

---

## ğŸ—ƒ Data Flow

1. **Frontend**: Splits book â†’ sends `chunks[]` to backend
2. **Backend**: Embeds & stores in Pinecone
3. **User**: Sends queries â†’ routed to appropriate agent via ADK
4. **ADK Agents**: Fetch context from Pinecone, respond via Gemini

---

## ğŸ§± File Structure

```
book-agent-backend/
â”œâ”€â”€ app.py
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ agents/                 # Google ADK agent logic
â”‚   â”œâ”€â”€ chat_agent.py
â”‚   â”œâ”€â”€ short_qa_agent.py
â”‚   â”œâ”€â”€ long_qa_agent.py
â”‚   â””â”€â”€ best_answer_agent.py
â”‚
â”œâ”€â”€ services/               # Core functionality
â”‚   â”œâ”€â”€ embedder.py
â”‚   â”œâ”€â”€ vector_store.py
â”‚   â””â”€â”€ retriever.py
â”‚
â”œâ”€â”€ routes/                 # Flask routes
â”‚   â”œâ”€â”€ upload.py
â”‚   â”œâ”€â”€ chat.py
â”‚   â””â”€â”€ qa.py
â”‚
â””â”€â”€ config/
    â””â”€â”€ settings.py
```

---

## ğŸ” Environment Variables

```bash
PINECONE_API_KEY=...
PINECONE_ENVIRONMENT=...
GOOGLE_API_KEY=...
PINECONE_INDEX_NAME=book-index
```

---

## ğŸ“Š Non-Functional Requirements

- â± Fast query response (< 2s)
- ğŸ§© Modular architecture
- ğŸ” Secure secrets and payload validation
- âš™ï¸ Retry mechanism for embedding & storage failures
- ğŸ§ª Unit tested endpoints
- ğŸ” Logging and error tracking
